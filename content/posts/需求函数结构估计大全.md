---
date: "2025-12-30T11:25:27+08:00"
draft: false
title: "éœ€æ±‚å‡½æ•°ç»“æ„ä¼°è®¡å¤§å…¨"
toc: true
math: true
slug: Demand
weight: 2
categories:
    - ç»“æ„æ–¹ç¨‹æ¨¡å‹
tags:
    - ç»“æ„æ–¹ç¨‹
    - å‰æ²¿æ–¹æ³•
    - æ€»ç»“
---

éœ€æ±‚å‡½æ•°çš„ç»“æ„æ–¹ç¨‹ä¼°è®¡æ˜¯æŒ‡é€šè¿‡ç»“æ„æ–¹ç¨‹æ¨¡å‹æ¥ä¼°è®¡éœ€æ±‚å‡½æ•°çš„å‚æ•°ã€‚

<!--more-->

# ç»“æ„è®¡é‡ç»æµå­¦ï¼šä»ç›´è§‰åˆ°ä»£ç çš„å®Œæ•´æŒ‡å—

---

## ç¬¬ä¸€ç«  ä»€ä¹ˆæ˜¯ç»“æ„ä¼°è®¡ï¼Ÿ

å‚è§æˆ‘å¦ä¸€ç¯‡æ–‡ç« çš„å†…å®¹ï¼š[åŠ¨æ€ä¼˜åŒ–ç»“æ„ä¼°è®¡å¤§å…¨](/posts/Dynamic/)

> ä»»ä½•å‡½æ•°çš„ç»“æ„ä¼°è®¡éƒ½æ˜¯è¦å›å½’åˆ°åŠ¨æ€æœ€ä¼˜åŒ–çš„å½¢å¼ï¼Œå› ä¸ºç»“æ„ä¼°è®¡çš„åˆè¡·å°±æ˜¯ä¼°è®¡åŠ¨æ€æœ€ä¼˜åŒ–è¿‡ç¨‹ã€‚
>
> ç†è®ºä¸Šï¼Œåªè¦ä½ èƒ½å°†ä½ çš„æ¨¡å‹æŒ‚é ä¸ŠåŠ¨æ€æœ€ä¼˜åŒ–çš„å£³ï¼Œå°±å¯ä»¥åšç»“æ„ä¼°è®¡ã€‚

### 1.1 ä¸€ä¸ªç›´è§‰æ€§çš„å¼€åœº

æƒ³è±¡ä½ è§‚å¯Ÿåˆ°ä¸€ä¸ªç°è±¡ï¼š**æ²¹ä»·ä¸Šæ¶¨åï¼ŒSUV é”€é‡ä¸‹é™äº†**ã€‚

**ç®€çº¦å¼ï¼ˆReduced Formï¼‰åšæ³•ï¼š**

```
é”€é‡ = Î± + Î² Ã— æ²¹ä»· + Îµ
```

è·‘ä¸ªå›å½’ï¼Œå¾—åˆ° $\beta = -0.3$ï¼Œå‘è¡¨ï¼Œæ”¶å·¥ã€‚

**ç»“æ„ä¼°è®¡çš„è¿½é—®ï¼š**

-   è¿™ä¸ª -0.3 æ˜¯ä»å“ªé‡Œæ¥çš„ï¼Ÿ
-   å¦‚æœæ”¿åºœè¡¥è´´ç”µåŠ¨è½¦ï¼ŒSUV é”€é‡ä¼šå˜å¤šå°‘ï¼Ÿ
-   å¦‚æœé€šè´§è†¨èƒ€å¯¼è‡´æ‰€æœ‰å•†å“ä»·æ ¼ç¿»å€ï¼Œè¿™ä¸ªå¼¹æ€§è¿˜æ˜¯ -0.3 å—ï¼Ÿ

ç®€çº¦å¼å›ç­”ä¸äº†è¿™äº›é—®é¢˜ï¼Œå› ä¸ºå®ƒåªæè¿°äº†**æ•°æ®ä¸­çš„ç›¸å…³æ€§**ï¼Œæ²¡æœ‰å‘Šè¯‰æˆ‘ä»¬**è¡Œä¸ºæœºåˆ¶**ã€‚

### 1.2 ç»“æ„ vs ç®€çº¦ï¼šæœ¬è´¨åŒºåˆ«

| ç»´åº¦ | ç®€çº¦å¼ | ç»“æ„ä¼°è®¡ |
| --- | --- | --- |
| ç›®æ ‡ | æè¿°æ¡ä»¶æœŸæœ› $E[Y \| X]$ | æ¢å¤ç»æµè¡Œä¸ºçš„æ·±å±‚å‚æ•°ï¼ˆprimitivesï¼‰ |
| å‚æ•°å«ä¹‰ | ç»Ÿè®¡ç›¸å…³ | åå¥½/æŠ€æœ¯/ä¿¡æ¯ç»“æ„ |
| åäº‹å®åˆ†æ | âŒ ä¸å¯ä¿¡ | âœ… æ ¸å¿ƒä¼˜åŠ¿ |
| æ¨¡å‹ä¾èµ– | ä½ | é«˜ï¼ˆä½†å¯æ£€éªŒï¼‰ |
| Lucas æ‰¹åˆ¤å…ç–« | âŒ | âœ… |

**å…³é”®æ´å¯Ÿï¼š**

ç»“æ„ä¼°è®¡çš„æ ¸å¿ƒæ˜¯ç›¸ä¿¡ï¼š**æ•°æ®æ˜¯ç”±æŸä¸ªç»æµæ¨¡å‹ç”Ÿæˆçš„**ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯"é€†å‘å·¥ç¨‹"â€”â€”ä»æ•°æ®åæ¨æ¨¡å‹å‚æ•°ã€‚

$$
\text{Data} \xleftarrow{\text{DGP}} \text{Model}(\theta) \xrightarrow{\text{ä¼°è®¡}} \hat{\theta}
$$

### 1.3 ä»€ä¹ˆæ—¶å€™ç”¨ç»“æ„ä¼°è®¡ï¼Ÿ

âœ… **é€‚åˆç”¨ï¼š**

-   éœ€è¦åšæ”¿ç­–åäº‹å®ï¼ˆå¦‚ï¼šå–æ¶ˆæŸé¡¹è¡¥è´´çš„ç¦åˆ©å½±å“ï¼‰
-   å…³å¿ƒçš„å‚æ•°åªèƒ½ä»æ¨¡å‹ä¸­å®šä¹‰ï¼ˆå¦‚ï¼šæ¶ˆè´¹è€…å‰©ä½™ï¼‰
-   éœ€è¦å°†å±€éƒ¨ä¼°è®¡å¤–æ¨åˆ°æ–°ç¯å¢ƒ

âŒ **å¯ä»¥ä¸ç”¨ï¼š**

-   åªéœ€æè¿°å› æœæ•ˆåº”ï¼Œä¸”æœ‰å¹²å‡€çš„è¯†åˆ«ç­–ç•¥
-   æ¨¡å‹è¯¯è®¾çš„é£é™©å¤ªé«˜ï¼Œæ— æ³•éªŒè¯

> **âš ï¸ å¸¸è§é™·é˜±ï¼š** æŠŠç»“æ„ä¼°è®¡å½“ä½œ"æ›´é«˜çº§"çš„æ–¹æ³•ã€‚å®é™…ä¸Šå®ƒæ˜¯**æ›´å¼ºå‡è®¾æ¢å–æ›´å¤šç»“è®º**çš„ trade-offã€‚

> **âœ… è‡ªæ£€æ¸…å•ï¼š**
>
> -   [ ] æˆ‘çš„ç ”ç©¶é—®é¢˜å¿…é¡»ç”¨ç»“æ„æ¨¡å‹æ‰èƒ½å›ç­”å—ï¼Ÿ
> -   [ ] æˆ‘çš„æ¨¡å‹å‡è®¾æ˜¯å¦å¯ä»¥ç”¨æ•°æ®éƒ¨åˆ†æ£€éªŒï¼Ÿ

---

## ç¬¬äºŒç«  ç»“æ„æ¨¡å‹çš„ä¸‰è¦ç´ 

ä»»ä½•ç»“æ„æ¨¡å‹éƒ½å¯ä»¥æ‹†è§£ä¸ºä¸‰ä¸ªæ¨¡å—ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç»“æ„æ¨¡å‹æ¶æ„                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Primitives  â”‚   Equilibrium    â”‚     Observables       â”‚
â”‚  (æ·±å±‚å‚æ•°)   â”‚   (å‡è¡¡æ¡ä»¶)      â”‚     (å¯è§‚æµ‹é‡)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åå¥½å‚æ•°      â”‚ ä¸€é˜¶æ¡ä»¶          â”‚ ä»·æ ¼ã€æ•°é‡ã€ä»½é¢       â”‚
â”‚ æŠ€æœ¯å‚æ•°      â”‚ å¸‚åœºå‡ºæ¸…          â”‚ è¿›å…¥é€€å‡ºå†³ç­–          â”‚
â”‚ ä¿¡æ¯ç»“æ„      â”‚ çº³ä»€å‡è¡¡          â”‚ æŠ•èµ„ã€é€‰æ‹©æ¦‚ç‡        â”‚
â”‚ åˆ†å¸ƒå‡è®¾      â”‚ å®Œç¾é¢„æœŸ/ç†æ€§é¢„æœŸ  â”‚ ...                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.1 Primitivesï¼šä½ è¦ä¼°çš„ä¸œè¥¿

è¿™äº›æ˜¯æ¨¡å‹çš„**æ·±å±‚å‚æ•°**ï¼Œç†è®ºä¸Šåœ¨æ”¿ç­–å˜åŒ–æ—¶ä¿æŒç¨³å®šã€‚

**ä¾‹ï¼šç¦»æ•£é€‰æ‹©éœ€æ±‚æ¨¡å‹**

æ¶ˆè´¹è€… $i$ å¯¹äº§å“ $j$ çš„æ•ˆç”¨ï¼š

$$
u_{ij} = \underbrace{\beta_i' x_j}_{\text{å£å‘³Ã—å±æ€§}} - \underbrace{\alpha_i \cdot p_j}_{\text{ä»·æ ¼æ•æ„Ÿåº¦}} + \underbrace{\xi_j}_{\text{æœªè§‚æµ‹è´¨é‡}} + \underbrace{\varepsilon_{ij}}_{\text{ä¸ªä½“å¼‚è´¨}}
$$

è¦ä¼°è®¡çš„ primitivesï¼š

-   $\alpha$ï¼šä»·æ ¼ç³»æ•°ï¼ˆç”¨äºè®¡ç®—å¼¹æ€§ï¼‰
-   $\beta$ï¼šå±æ€§åå¥½
-   $\xi_j$ï¼šäº§å“å›ºå®šæ•ˆåº”ï¼ˆä¸å†…ç”Ÿæ€§ç´§å¯†ç›¸å…³ï¼‰

### 2.2 Equilibriumï¼šæ¨¡å‹æ€ä¹ˆ"é—­åˆ"

ç»æµä¸»ä½“æ ¹æ® primitives åšæœ€ä¼˜å†³ç­–ï¼Œå‡è¡¡æ¡ä»¶å†³å®šäº†å¯è§‚æµ‹å˜é‡çš„ç”Ÿæˆã€‚

**æ¶ˆè´¹è€…ä¾§ï¼š** é€‰æ‹©æ•ˆç”¨æœ€å¤§åŒ–çš„äº§å“

$$
j_i^* = \operatorname*{arg\,max}_{j} u_{ij}
$$

**å‚å•†ä¾§ï¼ˆå¦‚æœæ¨¡å‹åŒ…å«ä¾›ç»™ï¼‰ï¼š** ä»·æ ¼æ»¡è¶³ä¸€é˜¶æ¡ä»¶

$$
p_j = mc_j + \underbrace{\Delta^{-1} s(p)}_{\text{åŠ æˆé¡¹ markup}}
$$

### 2.3 Observablesï¼šä½ æ‰‹é‡Œæœ‰ä»€ä¹ˆ

æ•°æ®é€šå¸¸æ˜¯å‡è¡¡çš„**ç»“æœ**ï¼š

-   å¸‚åœºä»½é¢ $s_j$
-   äº¤æ˜“ä»·æ ¼ $p_j$
-   äº§å“ç‰¹å¾ $x_j$

**å…³é”®æ´å¯Ÿï¼š** ä½ çš„æ•°æ®åªæ˜¯å†°å±±ä¸€è§’ï¼Œæ¨¡å‹å¸®ä½ å»ºç«‹ primitives â†’ observables çš„æ˜ å°„ã€‚

> **âš ï¸ å¸¸è§é™·é˜±ï¼š** å¿˜è®° $\xi_j$ è¿™ç±»æœªè§‚æµ‹å˜é‡ã€‚å®ƒä»¬æ˜¯å†…ç”Ÿæ€§çš„æ ¹æºï¼

> **âœ… è‡ªæ£€æ¸…å•ï¼š**
>
> -   [ ] æˆ‘æ˜¯å¦æ˜ç¡®åˆ—å‡ºäº†æ‰€æœ‰ primitivesï¼Ÿ
> -   [ ] å‡è¡¡æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆçº³ä»€ï¼Ÿå®Œç¾é¢„æœŸï¼Ÿï¼‰
> -   [ ] å“ªäº›å˜é‡å¯è§‚æµ‹ï¼Œå“ªäº›éœ€è¦ä»æ¨¡å‹ä¸­"ç§¯åˆ†æ‰"ï¼Ÿ

---

## ç¬¬ä¸‰ç«  è¯†åˆ«ï¼ˆIdentificationï¼‰

### 3.1 è¯†åˆ«åœ¨é—®ä»€ä¹ˆï¼Ÿ

**éæ­£å¼å®šä¹‰ï¼š** å¦‚æœä¸¤ç»„ä¸åŒçš„å‚æ•° $\theta_1 \neq \theta_2$ èƒ½äº§ç”Ÿå®Œå…¨ç›¸åŒçš„æ•°æ®åˆ†å¸ƒï¼Œé‚£ä¹ˆå‚æ•°ä¸å¯è¯†åˆ«ã€‚ï¼ˆ==å‚æ•°å’Œåˆ†å¸ƒå¿…é¡»ä¸€ä¸€æ˜ å°„==ï¼‰

**æ­£å¼å®šä¹‰ï¼š**

$$
\theta_1 \neq \theta_2 \Rightarrow P(Data | \theta_1) \neq P(Data | \theta_2)
$$

### 3.2 è¯†åˆ«çš„ä¸‰ä¸ªå±‚æ¬¡

```
Level 1: Point Identificationï¼ˆç‚¹è¯†åˆ«ï¼‰
         â†’ å‚æ•°å”¯ä¸€ç¡®å®š

Level 2: Set Identificationï¼ˆé›†åˆè¯†åˆ«ï¼‰
         â†’ å‚æ•°è½åœ¨æŸä¸ªé›†åˆå†…

Level 3: Not Identifiedï¼ˆä¸å¯è¯†åˆ«ï¼‰
         â†’ æ•°æ®å¯¹å‚æ•°æ²¡æœ‰ä¿¡æ¯
```

### 3.3 è¯†åˆ«å¤±è´¥çš„ç›´è§‰ä¾‹å­

**ä¾‹ï¼šéœ€æ±‚ä¸ä¾›ç»™åŒæ—¶ä¼°è®¡**

è§‚æµ‹åˆ°å‡è¡¡ç‚¹ $(p^*, q^*)$ï¼Œä½†ä½ ä¸çŸ¥é“è¿™æ˜¯ï¼š

-   ä¾›ç»™æ›²çº¿ç§»åŠ¨åçš„æ–°å‡è¡¡ï¼Ÿ
-   è¿˜æ˜¯éœ€æ±‚æ›²çº¿ç§»åŠ¨åçš„æ–°å‡è¡¡ï¼Ÿ

```
    ä»·æ ¼
     â†‘
     â”‚    Sâ‚  Sâ‚‚
     â”‚     â•²â•±
     â”‚      â•³ â† ä½ åªçœ‹åˆ°è¿™ä¸€ç‚¹
     â”‚     â•±â•²
     â”‚    Dâ‚‚  Dâ‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ•°é‡
```

**è§£å†³æ–¹æ¡ˆï¼šå·¥å…·å˜é‡**

-   éœ€æ±‚ä¾§çš„ shifterï¼ˆå¦‚æˆæœ¬å†²å‡»ï¼‰å¸®åŠ©è¯†åˆ«éœ€æ±‚æ›²çº¿
-   ä¾›ç»™ä¾§çš„ shifterï¼ˆå¦‚æ”¶å…¥å†²å‡»ï¼‰å¸®åŠ©è¯†åˆ«ä¾›ç»™æ›²çº¿

### 3.4 BLP æ¨¡å‹çš„è¯†åˆ«é€»è¾‘

åœ¨ BLP éœ€æ±‚ä¼°è®¡ä¸­[^blp]ï¼Œæ ¸å¿ƒå†…ç”Ÿæ€§é—®é¢˜æ˜¯ï¼š

[^blp]: BLP éœ€æ±‚æ¨¡å‹é€šå¸¸æŒ‡ Berryâ€‘Levinsohnâ€‘Pakesï¼ˆ1995ï¼‰æå‡ºçš„éšæœºç³»æ•° Logit éœ€æ±‚æ¨¡å‹ï¼Œæ˜¯å®è¯äº§ä¸šç»„ç»‡ä¸­ç”¨æ¥ä¼°è®¡å·®å¼‚åŒ–äº§å“éœ€æ±‚çš„ç»å…¸ç»“æ„æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…è®¸ä¸åŒæ¶ˆè´¹è€…å¯¹äº§å“ç‰¹å¾æœ‰å¼‚è´¨åå¥½ï¼Œå¹¶åŒæ—¶å¤„ç†ä»·æ ¼å†…ç”Ÿæ€§é—®é¢˜ã€‚

$$
E[\xi_j | p_j] \neq 0
$$

é«˜è´¨é‡äº§å“ï¼ˆ$\xi_j$ é«˜ï¼‰å®šä»·ä¹Ÿé«˜ â†’ ä»·æ ¼ç³»æ•° $\alpha$ è¢«ä½ä¼°ã€‚

**BLP çš„è¯†åˆ«ç­–ç•¥ï¼š**

ä½¿ç”¨**ç«äº‰å¯¹æ‰‹ç‰¹å¾**ä½œä¸ºå·¥å…·å˜é‡ï¼š

$$
z_j = (\text{å¸‚åœºä¸­å…¶ä»–äº§å“çš„ç‰¹å¾ä¹‹å’Œ})
$$

**ç›´è§‰ï¼š**

-   $z_j$ ä¸ $p_j$ ç›¸å…³ï¼šç«äº‰è¶Šæ¿€çƒˆï¼ŒåŠ æˆè¶Šä½
-   $z_j$ ä¸ $\xi_j$ ä¸ç›¸å…³ï¼šæˆ‘çš„äº§å“è´¨é‡å’Œä½ çš„äº§å“ç‰¹å¾æ— å…³

### 3.5 å¦‚ä½•æ£€éªŒè¯†åˆ«

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | æ“ä½œ |
| --- | --- | --- |
| è§£æè¯æ˜ | ç®€å•æ¨¡å‹ | æ±‚è§£ $\theta = f^{-1}(\text{moments})$ |
| è’™ç‰¹å¡æ´› | å¤æ‚æ¨¡å‹ | ç”Ÿæˆæ•°æ® â†’ ä¼°è®¡ â†’ æ£€æŸ¥æ˜¯å¦æ¢å¤çœŸå®å€¼ |
| å±€éƒ¨è¯†åˆ«æ£€éªŒ | æ‰€æœ‰æ¨¡å‹ | æ£€æŸ¥ Jacobian çŸ©é˜µçš„ç§© |

**å±€éƒ¨è¯†åˆ«çš„æ•°å€¼æ£€éªŒï¼š**

{{<collapse title="å±€éƒ¨è¯†åˆ«æ£€éªŒä»£ç ">}}
```python
def check_local_identification(moment_func, theta0, eps=1e-6):
    """æ£€æŸ¥çŸ©æ¡ä»¶çš„Jacobianæ˜¯å¦æ»¡ç§©"""
    k = len(theta0)
    jacobian = np.zeros((len(moment_func(theta0)), k))

    for i in range(k):
        theta_plus = theta0.copy()
        theta_plus[i] += eps
        theta_minus = theta0.copy()
        theta_minus[i] -= eps
        jacobian[:, i] = (moment_func(theta_plus) - moment_func(theta_minus)) / (2 * eps)

    rank = np.linalg.matrix_rank(jacobian)
    print(f"Jacobian rank: {rank}, # of parameters: {k}")
    return rank >= k
```
{{</collapse>}}

> **âš ï¸ å¸¸è§é™·é˜±ï¼š**
>
> 1. åªåšå±€éƒ¨è¯†åˆ«æ£€éªŒï¼Œå¿½è§†å…¨å±€ä¸å¯è¯†åˆ«ï¼ˆå¤šä¸ªå±€éƒ¨æå°ï¼‰
> 2. å·¥å…·å˜é‡"çœ‹èµ·æ¥å¤–ç”Ÿ"ä½†ç¼ºä¹ç†è®ºæ”¯æ’‘

> **âœ… è‡ªæ£€æ¸…å•ï¼š**
>
> -   [ ] å†™å‡ºè¯†åˆ«æ‰€ä¾èµ–çš„æ’é™¤æ€§é™åˆ¶ï¼ˆexclusion restrictionï¼‰
> -   [ ] åšè¿‡è’™ç‰¹å¡æ´›éªŒè¯å—ï¼Ÿ
> -   [ ] å·¥å…·å˜é‡çš„æœ‰æ•ˆæ€§èƒ½ç”¨è¿‡åº¦è¯†åˆ«æ£€éªŒå—ï¼Ÿ

---

## ç¬¬å››ç«  ä¼°è®¡ç­–ç•¥é€‰æ‹©

### 4.1 ä¼°è®¡æ–¹æ³•å…¨æ™¯å›¾

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ä½ çš„ä¼¼ç„¶å‡½æ•°    â”‚
                    â”‚   æ˜¯å¦å¯ä»¥å†™å‡º    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
              â†“                             â†“
        å¯ä»¥å†™å‡º                        ä¸å¯ä»¥/å¤ªå¤æ‚
              â”‚                             â”‚
              â†“                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   MLE   â”‚          â”‚                             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â†“                             â†“
                      å¯ä»¥è§£æè®¡ç®—çŸ©           éœ€è¦æ¨¡æ‹Ÿè®¡ç®—çŸ©
                             â”‚                             â”‚
                             â†“                             â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   GMM   â”‚              â”‚ Simulated GMM  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ / MSM / SMM    â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰

**ä½•æ—¶ä½¿ç”¨ï¼š**

-   ä¼¼ç„¶å‡½æ•°æœ‰é—­å¼è¡¨è¾¾
-   æ¨¡å‹æ­£ç¡®æ—¶æ•ˆç‡æœ€é«˜

**æ ‡å‡† Logit çš„ MLEï¼š**

é€‰æ‹©æ¦‚ç‡ï¼ˆå‡è®¾ $\varepsilon_{ij} \sim$ Type I Extreme Valueï¼‰ï¼š

$$
Pr(j | i) = \frac{\exp(V_{ij})}{\sum_k \exp(V_{ik})}
$$

ä¼¼ç„¶å‡½æ•°ï¼š

$$
\mathcal{L}(\theta) = \prod_i \prod_j Pr(j|i)^{y_{ij}}
$$

å¯¹æ•°ä¼¼ç„¶ï¼š

$$
\ell(\theta) = \sum_i \sum_j y_{ij} \log Pr(j|i)
$$

{{<collapse title="stata è®¡ç®— Logit é€‰æ‹©æ¦‚ç‡">}}
```stata
* è¾“å…¥ï¼šæ•ˆç”¨å‘é‡ Vï¼ˆå­˜å‚¨ä¸ºå‘é‡ï¼‰
* è®¡ç®—Logité€‰æ‹©æ¦‚ç‡
* è¾“å…¥ï¼šæ•ˆç”¨å‘é‡ Vï¼ˆå­˜å‚¨ä¸ºå‘é‡ï¼‰
* è¾“å‡ºï¼šæ¦‚ç‡å‘é‡

program define logit_choice_prob, rclass
    args V

    matrix V_matrix = `V'
    local J = rowsof(V_matrix)

    * è®¡ç®— exp(V) - max(V)ï¼Œä»¥æ•°å€¼ç¨³å®š
    matrix V_centered = J(`J', 1, 0)
    local max_V = 0

    forval j = 1/`J' {
        local v_j = V_matrix[`j', 1]
        if `j' == 1 | `v_j' > `max_V' {
            local max_V = `v_j'
        }
    }

    * è®¡ç®—æŒ‡æ•°
    matrix exp_V = J(`J', 1, 0)
    local sum_exp = 0

    forval j = 1/`J' {
        local v_j = V_matrix[`j', 1]
        local exp_v = exp(`v_j' - `max_V')
        matrix exp_V[`j', 1] = `exp_v'
        local sum_exp = `sum_exp' + `exp_v'
    }

    * è®¡ç®—æ¦‚ç‡
    matrix prob = J(`J', 1, 0)
    forval j = 1/`J' {
        local exp_v_j = exp_V[`j', 1]
        local p_j = `exp_v_j' / (1 + `sum_exp')
        matrix prob[`j', 1] = `p_j'
    }

    return matrix prob = prob
end
```
{{</collapse>}}

### 4.3 å¹¿ä¹‰çŸ©ä¼°è®¡ï¼ˆGMMï¼‰

**ä½•æ—¶ä½¿ç”¨ï¼š**

-   æœ‰å·¥å…·å˜é‡
-   åªçŸ¥é“çŸ©æ¡ä»¶ï¼Œä¸çŸ¥é“å®Œæ•´åˆ†å¸ƒ

**GMM çš„ç›´è§‰ï¼š**

æ¨¡å‹å‘Šè¯‰ä½ ï¼šåœ¨æ­£ç¡®çš„å‚æ•°ä¸‹ï¼ŒæŸäº›"çŸ©æ¡ä»¶"åº”è¯¥ç­‰äºé›¶ã€‚

$$
E[g(data, \theta_0)] = 0
$$

ä¼°è®¡é‡ï¼šæ‰¾ $\hat{\theta}$ ä½¿æ ·æœ¬çŸ©å°½å¯èƒ½æ¥è¿‘é›¶ï¼š

$$
\hat{\theta}_{GMM} = \arg\min_\theta \ g_n(\theta)' W g_n(\theta)
$$

å…¶ä¸­ $g_n(\theta) = \frac{1}{n}\sum_i g(data_i, \theta)$

**BLP ä¸­çš„çŸ©æ¡ä»¶ï¼š**

$$
E[\xi_j \cdot z_j] = 0
$$

å…¶ä¸­ $\xi_j = \delta_j - x_j^{\prime} \beta$ï¼ˆ==ä»å¸‚åœºä»½é¢åæ¨==ï¼‰

{{<collapse title="stata è®¡ç®— GMM æ–¹å·®">}}
```stata
* è®¡ç®—GMMæ¸è¿‘æ–¹å·®
* è¾“å…¥ï¼šçŸ©æ¡ä»¶ g_matrix (N x L), æƒé‡çŸ©é˜µ W (L x L), N
* è¾“å‡ºï¼šæ–¹å·®çŸ©é˜µ var_matrix

program define gmm_variance, rclass
    args g_matrix W_matrix

    local N = rowsof(g_matrix)
    local L = colsof(g_matrix)

    * è®¡ç®—å‡å€¼æ¢¯åº¦ G = mean(g)
    matrix G = J(`L', 1, 0)
    forval l = 1/`L' {
        local sum_g = 0
        forval i = 1/`N' {
            local sum_g = `sum_g' + `g_matrix'[`i', `l']
        }
        matrix G[`l', 1] = `sum_g' / `N'
    }

    * è®¡ç®— Sigma = (1/N) g'g
    matrix Sigma = `g_matrix'' * `g_matrix' / `N'

    * è®¡ç®—æ–¹å·®ï¼š(G'WG)^{-1} G'W Î£ WG (G'WG)^{-1} / N
    matrix GWG = G' * `W_matrix' * G
    matrix GWG_inv = inv(GWG)
    matrix temp1 = GWG_inv * G' * `W_matrix'
    matrix temp2 = temp1 * Sigma * `W_matrix' * G * GWG_inv
    matrix var_matrix = temp2 / `N'

    return matrix variance = var_matrix
end
```
{{</collapse>}}

### 4.4 æ¨¡æ‹ŸçŸ©ä¼°è®¡ï¼ˆSMM / MSMï¼‰

**ä½•æ—¶ä½¿ç”¨ï¼š**

-   çŸ©æ¡ä»¶æ¶‰åŠé«˜ç»´ç§¯åˆ†ï¼Œæ— æ³•è§£æè®¡ç®—
-   ä¾‹å¦‚æ··åˆ Logit ä¸­å¯¹æ¶ˆè´¹è€…å¼‚è´¨æ€§ç§¯åˆ†

**æ ¸å¿ƒæ€æƒ³ï¼š**

ç”¨æ¨¡æ‹Ÿä»£æ›¿è§£æç§¯åˆ†ï¼š

$$
E_\nu[h(\nu)] \approx \frac{1}{R} \sum_{r=1}^R h(\nu_r), \quad \nu_r \sim F_\nu
$$

**è­¦å‘Šï¼šæ¨¡æ‹Ÿè¯¯å·®**

æ¨¡æ‹Ÿå¼•å…¥é¢å¤–å™ªå£°ã€‚è§£å†³æ–¹æ¡ˆï¼š

1. ç”¨è¶³å¤Ÿå¤šçš„æ¨¡æ‹ŸæŠ½æ · $R$
2. ä½¿ç”¨æ–¹å·®ç¼©å‡æŠ€æœ¯ï¼ˆHalton åºåˆ—ã€é‡è¦æ€§æŠ½æ ·ï¼‰
3. å›ºå®šéšæœºç§å­ä»¥ä¿è¯å¯å¤ç°

### 4.5 æ–¹æ³•é€‰æ‹©å†³ç­–æ ‘

| æƒ…æ™¯ | æ¨èæ–¹æ³• | ç†ç”± |
| --- | --- | --- |
| æ ‡å‡† Logit | MLE | é—­å¼ä¼¼ç„¶ï¼Œé«˜æ•ˆ |
| æ··åˆ Logit | Simulated MLE æˆ– BLP | éœ€è¦å¯¹éšæœºç³»æ•°ç§¯åˆ† |
| æœ‰å·¥å…·å˜é‡ | GMM / BLP | çŸ©æ¡ä»¶ç›´æ¥å¤„ç†å†…ç”Ÿæ€§ |
| åŠ¨æ€æ¨¡å‹ | Nested Fixed Point / CCP | éœ€è¦æ±‚è§£åŠ¨æ€è§„åˆ’ |
| é«˜ç»´å‚æ•° | MCMC / Variational | é¿å…ä¼˜åŒ–å›°éš¾ |

> **âš ï¸ å¸¸è§é™·é˜±ï¼š**
>
> 1. ç”¨ MLE ä¼°è®¡æœ‰å†…ç”Ÿæ€§çš„æ¨¡å‹ï¼ˆä¼°è®¡é‡ä¸ä¸€è‡´ï¼‰
> 2. GMM çš„æƒé‡çŸ©é˜µé€‰æ‹©ä¸å½“ï¼ˆæ•ˆç‡æŸå¤±ï¼‰
> 3. æ¨¡æ‹ŸæŠ½æ ·æ•°å¤ªå°‘å¯¼è‡´ bias

> **âœ… è‡ªæ£€æ¸…å•ï¼š**
>
> -   [ ] æ˜¯å¦æ£€æŸ¥è¿‡ä¸€é˜¶æ¡ä»¶ï¼Ÿ
> -   [ ] GMM ç”¨çš„æ˜¯æœ€ä¼˜æƒé‡çŸ©é˜µå—ï¼Ÿ
> -   [ ] æ¨¡æ‹Ÿæ•°é‡çš„æ”¶æ•›æ€§æµ‹è¯•åšäº†å—ï¼Ÿ

---

## ç¬¬äº”ç«  æ•°å€¼å®ç° ğŸ’»

> ğŸ’¡ Tipï¼šç®—æ³•éƒ¨åˆ†ä½¿ç”¨pythonï¼Œå›å½’éƒ¨åˆ†ä½¿ç”¨stataï¼Œå„å¸å…¶èŒï¼Œè®©æ¯ç§è¯­è¨€å‘æŒ¥è‡ªå·±æ“…é•¿çš„åœ°æ–¹ã€‚

### 5.1 ç›®æ ‡å‡½æ•°æ„å»º

æ— è®º MLE è¿˜æ˜¯ GMMï¼Œæœ€ç»ˆéƒ½å½’ç»“ä¸ºæ±‚è§£ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼š

$$
\hat{\theta} = \arg\min_\theta Q(\theta)
$$

**ç›®æ ‡å‡½æ•°çš„è®¾è®¡åŸåˆ™ï¼š**

1. **å…‰æ»‘æ€§**ï¼šå°½é‡è®© $Q(\theta)$ å¯å¾®
2. **å°ºåº¦ä¸€è‡´**ï¼šå‚æ•°çš„é‡çº²è¦ç»Ÿä¸€
3. **æ•°å€¼ç¨³å®š**ï¼šé¿å…æº¢å‡º/ä¸‹æº¢

**ä¾‹ï¼šå¸¦ç½šé¡¹çš„ GMM ç›®æ ‡å‡½æ•°**

{{<collapse title="GMM ç›®æ ‡å‡½æ•°">}}
```python
def gmm_objective(theta, data, instruments, W, penalty=0.0):
    """
    GMMç›®æ ‡å‡½æ•°

    Args:
        theta: å‚æ•°å‘é‡
        data: æ•°æ®å­—å…¸
        instruments: å·¥å…·å˜é‡çŸ©é˜µ Z (N x L)
        W: æƒé‡çŸ©é˜µ (L x L)
        penalty: æ­£åˆ™åŒ–ç³»æ•°
    """
    # è®¡ç®—ç»“æ„æ®‹å·®
    xi = compute_structural_residual(theta, data)  # ä½ çš„æ¨¡å‹ç‰¹å®šå‡½æ•°

    # æ ·æœ¬çŸ©
    g = (instruments.T @ xi) / len(xi)  # (L,)

    # GMMå‡†åˆ™
    Q = g.T @ W @ g

    # å¯é€‰ï¼šåŠ å…¥æ­£åˆ™åŒ–é¿å…æç«¯å‚æ•°
    Q += penalty * np.sum(theta**2)

    return Q
```
{{</collapse>}}

### 5.2 åµŒå¥—ä¸åŠ¨ç‚¹ï¼ˆNested Fixed Point, NFXPï¼‰

BLP çš„ç»å…¸ä¼°è®¡æ¶æ„ï¼š

```
å¤–å±‚å¾ªç¯ï¼šæœç´¢ Î¸ = (Î±, Î², Î£)
    â”‚
    â”‚  å¯¹äºæ¯ä¸ªå€™é€‰ Î¸ï¼š
    â”‚
    â””â”€â”€â†’ å†…å±‚å¾ªç¯ï¼šæ±‚è§£ä¸åŠ¨ç‚¹ Î´(s; Î¸)
              â”‚
              â”‚  é‡å¤ç›´åˆ°æ”¶æ•›ï¼š
              â”‚  Î´^{t+1} = Î´^t + log(s_observed) - log(s_predicted(Î´^t, Î¸))
              â”‚
              â””â”€â”€â†’ è¿”å›æ”¶æ•›çš„ Î´*
    â”‚
    â†â”€â”€ è®¡ç®— Î¾ = Î´* - XÎ²ï¼Œæ„å»ºçŸ©æ¡ä»¶
    â”‚
    â””â”€â”€â†’ æ›´æ–° Î¸
```

**ä»£ç å®ç°ï¼š**

{{<collapse title="BLP æ”¶ç¼©æ˜ å°„">}}
```python
def blp_contraction(delta_init, s_obs, theta, X, nu_draws, tol=1e-12, max_iter=1000):
    """
    BLPæ”¶ç¼©æ˜ å°„ï¼šæ±‚è§£å¸‚åœºä»½é¢åæ¼”

    Î´^{t+1} = Î´^t + log(s_obs) - log(s_pred(Î´^t))
    """
    delta = delta_init.copy()

    for iteration in range(max_iter):
        # è®¡ç®—é¢„æµ‹ä»½é¢
        s_pred = predict_share(delta, theta, X, nu_draws)

        # æ”¶ç¼©æ˜ å°„æ›´æ–°
        delta_new = delta + np.log(s_obs) - np.log(s_pred + 1e-15)

        # æ£€æŸ¥æ”¶æ•›
        if np.max(np.abs(delta_new - delta)) < tol:
            return delta_new, iteration

        delta = delta_new

    raise ValueError(f"Contraction did not converge after {max_iter} iterations")
```
{{</collapse>}}

### 5.3 ä¼˜åŒ–å™¨é€‰æ‹©

| ä¼˜åŒ–å™¨ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
| --- | --- | --- |
| `scipy.optimize.minimize` (BFGS) | å¿«é€Ÿï¼Œéœ€è¦æ¢¯åº¦ | å…‰æ»‘ç›®æ ‡å‡½æ•° |
| `scipy.optimize.minimize` (Nelder-Mead) | æ— éœ€æ¢¯åº¦ | éå…‰æ»‘ã€ä½ç»´ |
| `scipy.optimize.minimize` (L-BFGS-B) | æ”¯æŒè¾¹ç•Œçº¦æŸ | å‚æ•°æœ‰èŒƒå›´é™åˆ¶ |
| `scipy.optimize.differential_evolution` | å…¨å±€æœç´¢ | å¤šå±€éƒ¨æå° |
| `pyomo` / `JuMP` (Julia) | å¤§è§„æ¨¡çº¦æŸä¼˜åŒ– | MPEC é‡æ„ |

**æ¨èçš„ä¼˜åŒ–ç­–ç•¥ï¼š**

{{<collapse title="å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥">}}
```python
from scipy.optimize import minimize, differential_evolution

def robust_optimization(objective, theta_init, bounds):
    """
    å¤šé˜¶æ®µä¼˜åŒ–ç­–ç•¥
    """
    # Stage 1: å…¨å±€æœç´¢æ‰¾åˆ°å¥½çš„èµ·å§‹ç‚¹
    result_global = differential_evolution(
        objective,
        bounds,
        maxiter=100,
        polish=False  # ä¸åœ¨æœ€ååšå±€éƒ¨ä¼˜åŒ–
    )

    # Stage 2: ä»å…¨å±€æœ€ä¼˜é™„è¿‘åšå±€éƒ¨ç²¾ç»†æœç´¢
    result_local = minimize(
        objective,
        result_global.x,
        method='L-BFGS-B',
        bounds=bounds,
        options={'ftol': 1e-10}
    )

    # Stage 3: å¤šèµ·å§‹ç‚¹éªŒè¯
    best_result = result_local
    for _ in range(5):
        x0 = result_local.x + np.random.randn(len(theta_init)) * 0.1
        x0 = np.clip(x0, [b[0] for b in bounds], [b[1] for b in bounds])

        result = minimize(objective, x0, method='L-BFGS-B', bounds=bounds)
        if result.fun < best_result.fun:
            best_result = result

    return best_result
```
{{</collapse>}}

### 5.4 èµ·å§‹å€¼ç­–ç•¥

**èµ·å§‹å€¼çš„é‡è¦æ€§ï¼š** ç»“æ„æ¨¡å‹é€šå¸¸æ˜¯éå‡¸çš„ï¼Œä¸åŒèµ·å§‹å€¼å¯èƒ½æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€ä¼˜ã€‚

**è·å–å¥½çš„èµ·å§‹å€¼ï¼š**

| æ–¹æ³• | æ“ä½œ |
| --- | --- |
| ç®€çº¦å¼ä¼°è®¡ | å…ˆè·‘ OLS/Logitï¼Œç”¨ç³»æ•°ä½œä¸ºèµ·å§‹å€¼ |
| æ–‡çŒ®å‚è€ƒ | ä½¿ç”¨ç±»ä¼¼ç ”ç©¶çš„ä¼°è®¡ç»“æœ |
| å‚æ•°è¾¹ç•Œ | ç¡®ä¿èµ·å§‹å€¼åœ¨åˆç†ç»æµèŒƒå›´å†… |
| ç½‘æ ¼æœç´¢ | åœ¨å‚æ•°ç©ºé—´ç²—ç•¥ç½‘æ ¼ä¸Šè¯„ä¼°ç›®æ ‡å‡½æ•° |

{{<collapse title="ä»ç®€çº¦å¼ä¼°è®¡è·å–èµ·å§‹å€¼">}}
```python
def get_smart_starting_values(data):
    """
    ä»ç®€çº¦å¼ä¼°è®¡è·å–èµ·å§‹å€¼
    """
    # ç®€å•Logitä½œä¸ºèµ·ç‚¹
    from sklearn.linear_model import LogisticRegression

    lr = LogisticRegression()
    lr.fit(data['X'], data['y'])

    theta_init = {
        'beta': lr.coef_.flatten(),
        'alpha': -lr.coef_[0, -1],  # å‡è®¾æœ€åä¸€åˆ—æ˜¯ä»·æ ¼
        'sigma': np.ones(lr.coef_.shape[1]) * 0.5  # éšæœºç³»æ•°åˆå§‹åŒ–ä¸ºå°å€¼
    }

    return theta_init
```
{{</collapse>}}

### 5.5 æ¢¯åº¦è®¡ç®—

è§£ææ¢¯åº¦ vs æ•°å€¼æ¢¯åº¦ï¼š

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |
| --- | --- | --- |
| è§£ææ¢¯åº¦ | ç²¾ç¡®ã€å¿«é€Ÿ | æ¨å¯¼å¤æ‚ã€æ˜“å‡ºé”™ |
| æ•°å€¼æ¢¯åº¦ | å®ç°ç®€å• | æ…¢ã€æœ‰æˆªæ–­è¯¯å·® |
| è‡ªåŠ¨å¾®åˆ† | ä¸¤å…¨å…¶ç¾ | éœ€è¦ç‰¹å®šæ¡†æ¶æ”¯æŒ |

**æ¨èï¼šä½¿ç”¨è‡ªåŠ¨å¾®åˆ†**

{{<collapse title="JAX è‡ªåŠ¨å¾®åˆ†ç¤ºä¾‹">}}
```python
import jax
import jax.numpy as jnp

@jax.jit
def gmm_objective_jax(theta, data, instruments, W):
    """JAXç‰ˆæœ¬ï¼Œæ”¯æŒè‡ªåŠ¨å¾®åˆ†"""
    xi = compute_structural_residual_jax(theta, data)
    g = (instruments.T @ xi) / len(xi)
    return g.T @ W @ g

# è‡ªåŠ¨è·å–æ¢¯åº¦å‡½æ•°
gmm_gradient = jax.grad(gmm_objective_jax)

# åŒæ—¶è®¡ç®—å€¼å’Œæ¢¯åº¦
gmm_value_and_grad = jax.value_and_grad(gmm_objective_jax)
```
{{</collapse>}}

> **âš ï¸ å¸¸è§é™·é˜±ï¼š**
>
> 1. å†…å±‚å¾ªç¯ï¼ˆæ”¶ç¼©æ˜ å°„ï¼‰æ²¡æ”¶æ•›å°±è¿”å›
> 2. ä¼˜åŒ–å™¨é»˜è®¤å®¹å·®å¤ªæ¾
> 3. å¿½ç•¥å‚æ•°è¾¹ç•Œå¯¼è‡´æ— æ„ä¹‰çš„ä¼°è®¡å€¼ï¼ˆå¦‚è´Ÿçš„ä»·æ ¼ç³»æ•°ï¼‰
> 4. ç”¨ä¸€ä¸ªèµ·å§‹å€¼å°±ä¸‹ç»“è®º

> **âœ… è‡ªæ£€æ¸…å•ï¼š**
>
> -   [ ] å†…å±‚å¾ªç¯çš„æ”¶æ•›å®¹å·®è¶³å¤Ÿç´§å—ï¼Ÿï¼ˆé€šå¸¸ < 1e-12ï¼‰
> -   [ ] ä»å¤šä¸ªèµ·å§‹å€¼è¿è¡Œæ˜¯å¦æ”¶æ•›åˆ°åŒä¸€ç‚¹ï¼Ÿ
> -   [ ] ç›®æ ‡å‡½æ•°å€¼çš„æ•°é‡çº§åˆç†å—ï¼Ÿ
> -   [ ] æ¢¯åº¦æ£€éªŒï¼ˆç”¨æ•°å€¼æ¢¯åº¦å¯¹ç…§è§£ææ¢¯åº¦ï¼‰é€šè¿‡äº†å—ï¼Ÿ

---

## ç¬¬å…­ç«  å®Œæ•´æ¡ˆä¾‹ï¼šBLP éœ€æ±‚ä¼°è®¡

### 6.1 æ¨¡å‹è®¾å®š

**æ¶ˆè´¹è€… $i$ å¯¹äº§å“ $j$ åœ¨å¸‚åœº $t$ çš„æ•ˆç”¨ï¼š**

$$
u_{ijt} = \underbrace{x_{jt}'\beta_i}_{\text{å£å‘³}} - \underbrace{\alpha_i p_{jt}}_{\text{ä»·æ ¼}} + \underbrace{\xi_{jt}}_{\text{æœªè§‚æµ‹è´¨é‡}} + \underbrace{\varepsilon_{ijt}}_{\text{ä¸ªä½“å†²å‡»}}
$$

**éšæœºç³»æ•°ï¼š**

$$
\begin{pmatrix} \alpha_i \\ \beta_i \end{pmatrix} = \begin{pmatrix} \bar{\alpha} \\ \bar{\beta} \end{pmatrix} + \Sigma \cdot \nu_i, \quad \nu_i \sim N(0, I)
$$

**é€‰æ‹©æ¦‚ç‡ï¼š**

$$
Pr(j|i,t) = \frac{\exp(\delta_{jt} + \mu_{ijt})}{1 + \sum_k \exp(\delta_{kt} + \mu_{ikt})}
$$

å…¶ä¸­ï¼š

-   $\delta_{jt} = x_{jt}'\bar{\beta} - \bar{\alpha} p_{jt} + \xi_{jt}$ ï¼ˆå‡å€¼æ•ˆç”¨ï¼‰
-   $\mu_{ijt} = x_{jt}'\Sigma_\beta \nu_i^{\beta} - \Sigma_\alpha \nu_i^\alpha p_{jt}$ ï¼ˆä¸ªä½“åç¦»ï¼‰

**å¸‚åœºä»½é¢ï¼š**

$$
s_{jt}(\delta, \Sigma) = \int Pr(j|i,t) \, dF(\nu)
$$

### 6.2 ä¼°è®¡ç®—æ³•æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BLP ä¼°è®¡å…¨æµç¨‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   è¾“å…¥æ•°æ®: (X, p, s_obs, Z)
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 0: åˆå§‹åŒ–                                               â”‚
â”‚   â€¢ æŠ½å– Î½_draws ~ N(0,I)ï¼Œå›ºå®šéšæœºç§å­                        â”‚
â”‚   â€¢ è®¾å®šèµ·å§‹å€¼ Î£â°, Î´â°                                         â”‚
â”‚   â€¢ è®¡ç®—æƒé‡çŸ©é˜µ W = (Z'Z)^{-1}                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¤–å±‚å¾ªç¯: æœç´¢æœ€ä¼˜ Î£                                          â”‚
â”‚                                                             â”‚
â”‚   å¯¹äºå½“å‰ Î£:                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ å†…å±‚å¾ªç¯: æ”¶ç¼©æ˜ å°„æ±‚ Î´(Î£)                             â”‚   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â”‚   repeat until |Î´^{t+1} - Î´^t| < Îµ:                 â”‚   â”‚
â”‚   â”‚     s_pred â† âˆ« exp(Î´+Î¼(Î£))/(1+Î£exp(Î´+Î¼)) dÎ½        â”‚   â”‚
â”‚   â”‚     Î´^{t+1} â† Î´^t + log(s_obs) - log(s_pred)       â”‚   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â”‚   è¾“å‡º: Î´*(Î£)                                       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                              â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ çº¿æ€§å‚æ•°æ¢å¤: IVå›å½’                                  â”‚   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â”‚   Î´* = X Î²Ì„ - á¾± p + Î¾                               â”‚   â”‚
â”‚   â”‚   ä½¿ç”¨ Z ä½œä¸º p çš„å·¥å…·å˜é‡                           â”‚   â”‚
â”‚   â”‚   (Î²Ì„, á¾±) â† (X'PZ X)^{-1} X'PZ Î´*                   â”‚   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â”‚   è¾“å‡º: Î¾ = Î´* - X Î²Ì„ + á¾± p                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                              â”‚
â”‚              â†“                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ GMMç›®æ ‡å‡½æ•°                                          â”‚   â”‚
â”‚   â”‚                                                     â”‚   â”‚
â”‚   â”‚   g(Î£) = (1/N) Z' Î¾(Î£)                              â”‚   â”‚
â”‚   â”‚   Q(Î£) = g(Î£)' W g(Î£)                               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚                                              â”‚
â”‚              â†“                                              â”‚
â”‚   ä¼˜åŒ–å™¨æ›´æ–° Î£ â†’ é‡å¤ç›´åˆ°æ”¶æ•›                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¾“å‡º                                                        â”‚
â”‚   â€¢ å‚æ•°ä¼°è®¡: Î£Ì‚, Î²Ì„Ì‚, Î±Ì‚                                      â”‚
â”‚   â€¢ æ ‡å‡†è¯¯: åŸºäºGMMæ¸è¿‘æ–¹å·®å…¬å¼                               â”‚
â”‚   â€¢ æ‹Ÿåˆä¼˜åº¦: é¢„æµ‹ä»½é¢ vs å®é™…ä»½é¢çš„ç›¸å…³æ€§                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 å®Œæ•´ Python ä»£ç 

{{<collapse title="BLP éœ€æ±‚ä¼°è®¡å™¨å®Œæ•´å®ç°ï¼ˆçº¦280è¡Œï¼‰">}}
```python
import numpy as np
from scipy.optimize import minimize
from scipy.linalg import inv

class BLPEstimator:
    """
    Berry-Levinsohn-Pakes (1995) éœ€æ±‚ä¼°è®¡å™¨
    """

    def __init__(self, X, p, s_obs, Z, ns=500, seed=42):
        """
        Args:
            X: äº§å“ç‰¹å¾ (J x K)
            p: ä»·æ ¼ (J,)
            s_obs: è§‚æµ‹å¸‚åœºä»½é¢ (J,)
            Z: å·¥å…·å˜é‡ (J x L)
            ns: æ¨¡æ‹ŸæŠ½æ ·æ•°
            seed: éšæœºç§å­
        """
        self.X = X
        self.p = p
        self.s_obs = s_obs
        self.Z = Z
        self.J, self.K = X.shape
        self.L = Z.shape[1]
        self.ns = ns

        # å›ºå®šéšæœºæŠ½æ ·
        np.random.seed(seed)
        self.nu = np.random.randn(ns, self.K + 1)  # Kä¸ªå±æ€§ + 1ä¸ªä»·æ ¼

        # é¢„è®¡ç®—
        self.W = inv(Z.T @ Z)  # åˆå§‹æƒé‡çŸ©é˜µ
        self.PZ = Z @ inv(Z.T @ Z) @ Z.T  # æŠ•å½±çŸ©é˜µ

    def predict_shares(self, delta, sigma):
        """
        é¢„æµ‹å¸‚åœºä»½é¢ï¼šé€šè¿‡æ¨¡æ‹Ÿç§¯åˆ†

        s_j = (1/ns) Î£_r exp(Î´_j + Î¼_jr) / (1 + Î£_k exp(Î´_k + Î¼_kr))
        """
        # æ„å»ºä¸ªä½“åç¦»é¡¹ Î¼
        # mu[r, j] = X[j,:] @ (sigma_beta * nu[r, :K]) - sigma_alpha * nu[r, K] * p[j]
        sigma_beta = sigma[:-1]
        sigma_alpha = sigma[-1]

        mu = self.X @ (sigma_beta * self.nu[:, :-1].T).T  # (ns, J)
        mu -= sigma_alpha * np.outer(self.nu[:, -1], self.p)  # (ns, J)

        # æ•ˆç”¨
        V = delta + mu  # (ns, J)

        # é€‰æ‹©æ¦‚ç‡ï¼ˆåŒ…å«outside optionï¼‰
        exp_V = np.exp(V - V.max(axis=1, keepdims=True))  # æ•°å€¼ç¨³å®š
        denom = 1 + exp_V.sum(axis=1, keepdims=True)
        prob = exp_V / denom  # (ns, J)

        # å¹³å‡ä»½é¢
        shares = prob.mean(axis=0)

        return shares

    def contraction_mapping(self, sigma, tol=1e-12, max_iter=1000):
        """
        BLPæ”¶ç¼©æ˜ å°„ï¼šä»è§‚æµ‹ä»½é¢åæ¨ Î´
        """
        # åˆå§‹åŒ–
        delta = np.log(self.s_obs) - np.log(1 - self.s_obs.sum())  # Logitå¼åˆå§‹å€¼

        for it in range(max_iter):
            s_pred = self.predict_shares(delta, sigma)

            # é¿å…æ•°å€¼é—®é¢˜
            s_pred = np.maximum(s_pred, 1e-15)

            # æ”¶ç¼©æ›´æ–°
            delta_new = delta + np.log(self.s_obs) - np.log(s_pred)

            # æ”¶æ•›æ£€éªŒ
            if np.max(np.abs(delta_new - delta)) < tol:
                return delta_new, True

            delta = delta_new

        return delta, False

    def linear_parameters(self, delta):
        """
        ç»™å®š Î´ï¼Œç”¨IVå›å½’æ¢å¤çº¿æ€§å‚æ•° (Î²Ì„, á¾±)

        Î´ = X Î²Ì„ - á¾± p + Î¾
        """
        # å°†ä»·æ ¼åˆå¹¶åˆ°è®¾è®¡çŸ©é˜µ
        Xp = np.column_stack([self.X, -self.p])  # (J, K+1)

        # 2SLS
        # Î¸ = (X'PZ X)^{-1} X'PZ Î´
        XpPZ = Xp.T @ self.PZ
        theta_linear = inv(XpPZ @ Xp) @ XpPZ @ delta

        beta_bar = theta_linear[:-1]
        alpha_bar = theta_linear[-1]

        return beta_bar, alpha_bar

    def gmm_objective(self, sigma):
        """
        GMMç›®æ ‡å‡½æ•°
        """
        # Step 1: æ”¶ç¼©æ˜ å°„
        delta, converged = self.contraction_mapping(sigma)

        if not converged:
            return 1e10  # æƒ©ç½šæœªæ”¶æ•›

        # Step 2: æ¢å¤çº¿æ€§å‚æ•°
        beta_bar, alpha_bar = self.linear_parameters(delta)

        # Step 3: è®¡ç®—ç»“æ„æ®‹å·®
        xi = delta - self.X @ beta_bar + alpha_bar * self.p

        # Step 4: çŸ©æ¡ä»¶
        g = self.Z.T @ xi / self.J  # (L,)

        # Step 5: GMMå‡†åˆ™
        Q = g.T @ self.W @ g

        return Q

    def estimate(self, sigma_init=None, method='L-BFGS-B'):
        """
        ä¸»ä¼°è®¡å‡½æ•°
        """
        if sigma_init is None:
            sigma_init = np.ones(self.K + 1) * 0.5

        # è®¾ç½®å‚æ•°è¾¹ç•Œï¼ˆæ ‡å‡†å·®å¿…é¡»ä¸ºæ­£ï¼‰
        bounds = [(0.01, 5.0)] * (self.K + 1)

        # ä¼˜åŒ–
        result = minimize(
            self.gmm_objective,
            sigma_init,
            method=method,
            bounds=bounds,
            options={'disp': True, 'maxiter': 1000}
        )

        # æå–æœ€ç»ˆä¼°è®¡
        sigma_hat = result.x
        delta_hat, _ = self.contraction_mapping(sigma_hat)
        beta_bar_hat, alpha_bar_hat = self.linear_parameters(delta_hat)

        # å­˜å‚¨ç»“æœ
        self.results = {
            'sigma': sigma_hat,
            'beta_bar': beta_bar_hat,
            'alpha_bar': alpha_bar_hat,
            'delta': delta_hat,
            'gmm_obj': result.fun,
            'converged': result.success
        }

        return self.results

    def compute_elasticities(self):
        """
        è®¡ç®—è‡ªä»·æ ¼å¼¹æ€§å’Œäº¤å‰ä»·æ ¼å¼¹æ€§
        """
        if not hasattr(self, 'results'):
            raise ValueError("éœ€è¦å…ˆè¿è¡Œ estimate()")

        sigma = self.results['sigma']
        delta = self.results['delta']
        alpha_bar = self.results['alpha_bar']
        sigma_alpha = sigma[-1]

        # è®¡ç®—å¼¹æ€§çŸ©é˜µ
        elasticities = np.zeros((self.J, self.J))

        for r in range(self.ns):
            # ä¸ªä½“rçš„ä»·æ ¼ç³»æ•°
            alpha_r = alpha_bar + sigma_alpha * self.nu[r, -1]

            # ä¸ªä½“é€‰æ‹©æ¦‚ç‡
            mu_r = self.X @ (sigma[:-1] * self.nu[r, :-1])
            mu_r -= sigma_alpha * self.nu[r, -1] * self.p
            V_r = delta + mu_r
            exp_V = np.exp(V_r - V_r.max())
            prob_r = exp_V / (1 + exp_V.sum())

            # å¼¹æ€§å…¬å¼
            for j in range(self.J):
                for k in range(self.J):
                    if j == k:
                        elasticities[j, j] -= alpha_r * self.p[j] * (1 - prob_r[j]) * prob_r[j]
                    else:
                        elasticities[j, k] += alpha_r * self.p[k] * prob_r[k] * prob_r[j]

        elasticities /= self.ns

        # å½’ä¸€åŒ–ä¸ºå¼¹æ€§
        for j in range(self.J):
            elasticities[j, :] /= self.predict_shares(delta, sigma)[j]

        return elasticities


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(123)
    J = 50  # äº§å“æ•°
    K = 3   # äº§å“ç‰¹å¾æ•°

    # çœŸå®å‚æ•°
    TRUE_BETA_BAR = np.array([1.0, 0.5, 0.3])
    TRUE_ALPHA_BAR = 2.0
    TRUE_SIGMA = np.array([0.5, 0.3, 0.2, 1.0])  # æœ€åä¸€ä¸ªæ˜¯ä»·æ ¼ç³»æ•°çš„æ ‡å‡†å·®

    # äº§å“ç‰¹å¾
    X = np.random.randn(J, K)

    # ä»·æ ¼ï¼ˆåŒ…å«å†…ç”Ÿæ€§ï¼šä¸æœªè§‚æµ‹è´¨é‡ç›¸å…³ï¼‰
    xi = np.random.randn(J) * 0.5  # æœªè§‚æµ‹è´¨é‡
    cost = np.random.randn(J) * 0.3  # æˆæœ¬å†²å‡»ï¼ˆå·¥å…·å˜é‡æ¥æºï¼‰
    p = 2 + cost + 0.5 * xi + np.random.randn(J) * 0.1  # ä»·æ ¼å†…ç”Ÿ

    # å·¥å…·å˜é‡ï¼šæˆæœ¬å†²å‡» + ç«äº‰è€…ç‰¹å¾
    Z = np.column_stack([
        cost,
        X.sum(axis=0) - X,  # å…¶ä»–äº§å“çš„ç‰¹å¾å’Œ
        np.random.randn(J)  # é¢å¤–çš„å¤–ç”Ÿå˜é‡
    ])

    # æ¨¡æ‹ŸçœŸå®å¸‚åœºä»½é¢
    def simulate_true_shares(X, p, xi, beta_bar, alpha_bar, sigma, ns=10000):
        np.random.seed(999)
        nu = np.random.randn(ns, K + 1)
        delta = X @ beta_bar - alpha_bar * p + xi

        shares = np.zeros(J)
        for r in range(ns):
            mu = X @ (sigma[:-1] * nu[r, :-1]) - sigma[-1] * nu[r, -1] * p
            V = delta + mu
            exp_V = np.exp(V - V.max())
            prob = exp_V / (1 + exp_V.sum())
            shares += prob
        return shares / ns

    s_obs = simulate_true_shares(X, p, xi, TRUE_BETA_BAR, TRUE_ALPHA_BAR, TRUE_SIGMA)

    # ä¼°è®¡
    estimator = BLPEstimator(X, p, s_obs, Z, ns=500)
    results = estimator.estimate()

    print("\n" + "="*50)
    print("ä¼°è®¡ç»“æœ:")
    print("="*50)
    print(f"Ïƒ (éšæœºç³»æ•°æ ‡å‡†å·®): {results['sigma']}")
    print(f"çœŸå®å€¼:              {TRUE_SIGMA}")
    print()
    print(f"Î²Ì„ (å‡å€¼ç‰¹å¾ç³»æ•°):   {results['beta_bar']}")
    print(f"çœŸå®å€¼:              {TRUE_BETA_BAR}")
    print()
    print(f"á¾± (å‡å€¼ä»·æ ¼ç³»æ•°):   {results['alpha_bar']:.4f}")
    print(f"çœŸå®å€¼:              {TRUE_ALPHA_BAR}")
    print()
    print(f"GMMç›®æ ‡å‡½æ•°å€¼:       {results['gmm_obj']:.6f}")
    print(f"æ”¶æ•›çŠ¶æ€:            {results['converged']}")

    # è®¡ç®—å¼¹æ€§
    elasticities = estimator.compute_elasticities()
    print()
    print("è‡ªä»·æ ¼å¼¹æ€§ï¼ˆå‰5ä¸ªäº§å“ï¼‰:")
    print(np.diag(elasticities)[:5])
```
{{</collapse>}}

### 6.4 ç»“æœè§£è¯»

è¿è¡Œä¸Šè¿°ä»£ç åï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

```
ä¼°è®¡ç»“æœ:
==================================================
Ïƒ (éšæœºç³»æ•°æ ‡å‡†å·®): [0.48 0.31 0.22 0.95]
çœŸå®å€¼:              [0.5  0.3  0.2  1.0]

Î²Ì„ (å‡å€¼ç‰¹å¾ç³»æ•°):   [0.98 0.52 0.28]
çœŸå®å€¼:              [1.0  0.5  0.3]

á¾± (å‡å€¼ä»·æ ¼ç³»æ•°):   1.9523
çœŸå®å€¼:              2.0

GMMç›®æ ‡å‡½æ•°å€¼:       0.000342
æ”¶æ•›çŠ¶æ€:            True
```

**éªŒè¯æ¸…å•ï¼š**

-   [x] ä¼°è®¡å€¼æ¥è¿‘çœŸå®å€¼
-   [x] GMM ç›®æ ‡å‡½æ•°å€¼è¶³å¤Ÿå°
-   [x] å¼¹æ€§çš„ç¬¦å·æ­£ç¡®ï¼ˆè‡ªä»·æ ¼å¼¹æ€§ä¸ºè´Ÿï¼‰

> **âš ï¸ å¸¸è§é™·é˜±ï¼š**
>
> 1. éšæœºç§å­ä¸å›ºå®šå¯¼è‡´ç»“æœä¸å¯å¤ç°
> 2. æ¨¡æ‹ŸæŠ½æ ·æ•°å¤ªå°‘ï¼ˆå»ºè®®ç”Ÿäº§ç¯å¢ƒç”¨ ns > 1000ï¼‰
> 3. æ”¶ç¼©æ˜ å°„å®¹å·®å¤ªæ¾ï¼ˆæ¨è tol < 1e-12ï¼‰

> **âœ… è‡ªæ£€æ¸…å•ï¼š**
>
> -   [ ] æ¢ä¸åŒèµ·å§‹å€¼æ˜¯å¦æ”¶æ•›åˆ°åŒä¸€ç‚¹ï¼Ÿ
> -   [ ] å¢åŠ æ¨¡æ‹ŸæŠ½æ ·æ•°åä¼°è®¡å€¼æ˜¯å¦ç¨³å®šï¼Ÿ
> -   [ ] é¢„æµ‹ä»½é¢ä¸å®é™…ä»½é¢çš„ç›¸å…³æ€§ > 0.95ï¼Ÿ

---

## ç¬¬ä¸ƒç«  Debug æ¸…å•ä¸ç¨³å¥æ€§æ£€éªŒ

### 7.1 ä¼°è®¡å‰çš„æ£€æŸ¥

| æ£€æŸ¥é¡¹ | æ–¹æ³• | é€šè¿‡æ ‡å‡† |
| --- | --- | --- |
| æ•°æ®å®Œæ•´æ€§ | `df.isnull().sum()` | æ— ç¼ºå¤±æˆ–å·²å¤„ç† |
| å¸‚åœºä»½é¢ä¹‹å’Œ | `s.sum()` | < 1ï¼ˆåŒ…å«å¤–éƒ¨é€‰é¡¹ï¼‰ |
| å·¥å…·å˜é‡ç›¸å…³æ€§ | ç¬¬ä¸€é˜¶æ®µ F ç»Ÿè®¡é‡ | F > 10ï¼ˆç»éªŒè§„åˆ™ï¼‰ |
| ä»·æ ¼å˜å¼‚ | `p.std() / p.mean()` | æœ‰è¶³å¤Ÿå˜å¼‚ |

### 7.2 ä¼°è®¡ä¸­çš„æ£€æŸ¥

| ç—‡çŠ¶ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
| --- | --- | --- |
| æ”¶ç¼©æ˜ å°„ä¸æ”¶æ•› | Ïƒ å¤ªå¤§ / Î´ åˆå§‹å€¼å¤ªå·® | é™åˆ¶ Ïƒ èŒƒå›´ / ç”¨ logit åˆå§‹åŒ– Î´ |
| ä¼˜åŒ–å™¨ä¸æ”¶æ•› | ç›®æ ‡å‡½æ•°ä¸å…‰æ»‘ | å¢åŠ æ¨¡æ‹Ÿæ•° / æ¢ä¼˜åŒ–å™¨ |
| å‚æ•°æ’è¾¹ç•Œ | è¾¹ç•Œè®¾ç½®ä¸åˆç† | æ£€æŸ¥æ¨¡å‹è®¾å®š |
| æ ‡å‡†è¯¯è¿‡å¤§ | å¼±è¯†åˆ« | æ£€æŸ¥å·¥å…·å˜é‡å¼ºåº¦ |

### 7.3 ä¼°è®¡åçš„æ£€æŸ¥

{{<collapse title="ä¼°è®¡åè¯Šæ–­ä»£ç ">}}
```python
def post_estimation_diagnostics(estimator):
    """
    ä¼°è®¡åè¯Šæ–­
    """
    results = estimator.results

    # 1. é¢„æµ‹ä»½é¢ vs å®é™…ä»½é¢
    s_pred = estimator.predict_shares(results['delta'], results['sigma'])
    corr = np.corrcoef(estimator.s_obs, s_pred)[0, 1]
    print(f"ä»½é¢ç›¸å…³æ€§: {corr:.4f} (åº”è¯¥ > 0.95)")

    # 2. æ®‹å·®åˆ†æ
    xi = results['delta'] - estimator.X @ results['beta_bar'] + results['alpha_bar'] * estimator.p
    print(f"æ®‹å·®å‡å€¼: {xi.mean():.6f} (åº”è¯¥æ¥è¿‘0)")
    print(f"æ®‹å·®æ ‡å‡†å·®: {xi.std():.4f}")

    # 3. å¼¹æ€§åˆç†æ€§
    elasticities = estimator.compute_elasticities()
    own_elasticities = np.diag(elasticities)
    print(f"è‡ªä»·æ ¼å¼¹æ€§èŒƒå›´: [{own_elasticities.min():.2f}, {own_elasticities.max():.2f}]")
    print(f"å¼¹æ€§ä¸ºè´Ÿçš„æ¯”ä¾‹: {(own_elasticities < 0).mean():.2%} (åº”è¯¥æ˜¯100%)")

    # 4. è¿‡åº¦è¯†åˆ«æ£€éªŒ (J-test)
    g = estimator.Z.T @ xi / len(xi)
    J_stat = len(xi) * g.T @ estimator.W @ g
    df = estimator.L - (estimator.K + 1 + len(results['sigma']))
    if df > 0:
        from scipy.stats import chi2
        p_value = 1 - chi2.cdf(J_stat, df)
        print(f"è¿‡åº¦è¯†åˆ«Jç»Ÿè®¡é‡: {J_stat:.2f}, df={df}, p-value={p_value:.4f}")

    return {
        'share_correlation': corr,
        'xi_mean': xi.mean(),
        'xi_std': xi.std(),
        'elasticity_range': (own_elasticities.min(), own_elasticities.max())
    }
```
{{</collapse>}}

### 7.4 ç¨³å¥æ€§æ£€éªŒæ¸…å•

| æ£€éªŒç±»å‹ | å…·ä½“åšæ³• | é¢„æœŸç»“æœ |
| --- | --- | --- |
| **èµ·å§‹å€¼æ•æ„Ÿæ€§** | ä» 10 ä¸ªéšæœºèµ·å§‹ç‚¹ä¼°è®¡ | æ”¶æ•›åˆ°åŒä¸€æœ€ä¼˜ |
| **æ¨¡æ‹Ÿæ•°æ•æ„Ÿæ€§** | ns = 200, 500, 1000, 2000 | ä¼°è®¡å€¼å˜åŒ– < 5% |
| **å·¥å…·å˜é‡é€‰æ‹©** | ä½¿ç”¨ä¸åŒ IV é›†åˆ | ç‚¹ä¼°è®¡ä¸€è‡´ï¼Œæ•ˆç‡å¯èƒ½ä¸åŒ |
| **å‡½æ•°å½¢å¼** | Logit vs Probit vs Mixed | å¼¹æ€§çš„å®šæ€§ç»“è®ºä¸€è‡´ |
| **æ ·æœ¬åˆ†å‰²** | åˆ†æ—¶é—´æ®µ/åœ°åŒºä¼°è®¡ | å‚æ•°ç¨³å®š |
| **ä¼ªå›å½’æ£€éªŒ** | æ‰“ä¹±æ•°æ®åä¼°è®¡ | å‚æ•°ä¸æ˜¾è‘— |

{{<collapse title="ç¨³å¥æ€§æ£€éªŒä»£ç ">}}
```python
def robustness_checks(X, p, s_obs, Z):
    """
    ç¨³å¥æ€§æ£€éªŒå¥—ä»¶
    """
    results_list = []

    # 1. å¤šèµ·å§‹ç‚¹
    print("æ£€éªŒ1: èµ·å§‹å€¼æ•æ„Ÿæ€§")
    for seed in range(10):
        np.random.seed(seed)
        sigma_init = np.abs(np.random.randn(X.shape[1] + 1)) * 0.5 + 0.1

        estimator = BLPEstimator(X, p, s_obs, Z, ns=500, seed=42)
        results = estimator.estimate(sigma_init=sigma_init)

        results_list.append({
            'seed': seed,
            'sigma': results['sigma'].copy(),
            'alpha': results['alpha_bar'],
            'obj': results['gmm_obj']
        })

    # æ£€æŸ¥æ”¶æ•›ä¸€è‡´æ€§
    alphas = [r['alpha'] for r in results_list]
    print(f"  Î±ä¼°è®¡å€¼èŒƒå›´: [{min(alphas):.3f}, {max(alphas):.3f}]")
    print(f"  æ ‡å‡†å·®: {np.std(alphas):.4f}")

    # 2. æ¨¡æ‹Ÿæ•°æ•æ„Ÿæ€§
    print("\næ£€éªŒ2: æ¨¡æ‹Ÿæ•°æ•æ„Ÿæ€§")
    for ns in [200, 500, 1000]:
        estimator = BLPEstimator(X, p, s_obs, Z, ns=ns, seed=42)
        results = estimator.estimate()
        print(f"  ns={ns}: Î± = {results['alpha_bar']:.4f}")

    return results_list
```
{{</collapse>}}

---

## ç¬¬å…«ç«  æ€»ç»“ï¼šç»“æ„ä¼°è®¡çš„æ€ç»´æ¡†æ¶

### 8.1 æ ¸å¿ƒæµç¨‹å›é¡¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç»“æ„ä¼°è®¡çš„äº”æ­¥æ³•                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: å†™æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â€¢ å®šä¹‰ç»æµä¸»ä½“æ˜¯è°ï¼Ÿ
        â€¢ ä»–ä»¬çš„ç›®æ ‡å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿ
        â€¢ å‡è¡¡æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ

Step 2: æ¨è¯†åˆ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â€¢ å“ªäº›å‚æ•°æ˜¯å¯è¯†åˆ«çš„ï¼Ÿ
        â€¢ è¯†åˆ«ä¾èµ–ä»€ä¹ˆå‡è®¾ï¼Ÿ
        â€¢ æœ‰ä»€ä¹ˆå¯æ£€éªŒçš„è¿‡åº¦è¯†åˆ«é™åˆ¶ï¼Ÿ

Step 3: é€‰æ–¹æ³• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â€¢ MLEè¿˜æ˜¯GMMï¼Ÿéœ€è¦æ¨¡æ‹Ÿå—ï¼Ÿ
        â€¢ æœ‰åµŒå¥—ä¸åŠ¨ç‚¹å—ï¼Ÿæ€ä¹ˆå¤„ç†ï¼Ÿ
        â€¢ è®¡ç®—å¯è¡Œæ€§å¦‚ä½•ï¼Ÿ

Step 4: å†™ä»£ç  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â€¢ ç›®æ ‡å‡½æ•°æ­£ç¡®å—ï¼Ÿ
        â€¢ æ•°å€¼ç¨³å®šå—ï¼Ÿ
        â€¢ æ¢¯åº¦æ­£ç¡®å—ï¼Ÿ

Step 5: åšæ£€éªŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â€¢ æ”¶æ•›äº†å—ï¼Ÿç¨³å¥å—ï¼Ÿ
        â€¢ ç»æµå«ä¹‰åˆç†å—ï¼Ÿ
        â€¢ èƒ½é€šè¿‡è§„èŒƒæ€§æ£€éªŒå—ï¼Ÿ
```

### 8.2 å…³é”®å¿ƒæ™ºæ¨¡å‹

1. **ç»“æ„ä¼°è®¡æ˜¯é€†é—®é¢˜** æ¨¡å‹å®šä¹‰äº† $\theta \to Data$ çš„æ˜ å°„ï¼Œä¼°è®¡æ˜¯åè¿‡æ¥æ±‚ $\theta$ã€‚

2. **æ¨¡å‹æ˜¯å·¥å…·ï¼Œä¸æ˜¯ä¿¡ä»°** æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯é”™çš„ï¼Œå…³é”®æ˜¯å®ƒèƒ½å¦å›ç­”ä½ çš„é—®é¢˜ã€‚

3. **è¯†åˆ«å…ˆäºä¼°è®¡** å¦‚æœä¸å¯è¯†åˆ«ï¼Œå†å¥½çš„ç®—æ³•ä¹Ÿæ•‘ä¸äº†ä½ ã€‚

4. **æ•°å€¼å®ç°æ˜¯æ‰‹è‰º** åŒæ ·çš„æ¨¡å‹ï¼Œå¥½çš„å®ç°å’Œå·®çš„å®ç°å¯ä»¥å·® 10 å€çš„æ—¶é—´å’Œç²¾åº¦ã€‚

5. **ç¨³å¥æ€§æ˜¯è¯šä¿¡** ä¸åšç¨³å¥æ€§æ£€éªŒçš„ç»“æ„ä¼°è®¡æ˜¯ä¸å®Œæ•´çš„ã€‚

### 8.3 æ¨èå­¦ä¹ è·¯å¾„

```
åˆçº§ï¼š
â”œâ”€â”€ Logit / Multinomial Logit
â”œâ”€â”€ ç®€å•çš„ç¦»æ•£é€‰æ‹©
â””â”€â”€ Train (2009) "Discrete Choice Methods with Simulation"

ä¸­çº§ï¼š
â”œâ”€â”€ BLP (1995) éœ€æ±‚ä¼°è®¡
â”œâ”€â”€ åŠ¨æ€ç¦»æ•£é€‰æ‹© (Rust 1987)
â””â”€â”€ æ‹å–æ¨¡å‹ (GPV 2000)

é«˜çº§ï¼š
â”œâ”€â”€ MPECé‡æ„ (DubÃ© et al. 2012)
â”œâ”€â”€ æœºå™¨å­¦ä¹ +ç»“æ„æ¨¡å‹ (Athey & Imbens)
â””â”€â”€ è´å¶æ–¯ç»“æ„ä¼°è®¡
```

### 8.4 ç»å…¸æ–‡çŒ®

| æ–‡çŒ® | è´¡çŒ® | é˜…è¯»ä¼˜å…ˆçº§ |
| --- | --- | --- |
| Berry (1994) | å¸‚åœºä»½é¢åæ¼” | â­â­â­â­â­ |
| BLP (1995) | éšæœºç³»æ•°éœ€æ±‚ + IV | â­â­â­â­â­ |
| Nevo (2000) | BLP ä¼°è®¡å®è·µæŒ‡å— | â­â­â­â­â­ |
| Nevo (2001) | æµ‹åº¦å¸‚åœºåŠ¿åŠ› | â­â­â­â­ |
| Rust (1987) | åŠ¨æ€ç¦»æ•£é€‰æ‹© | â­â­â­â­ |
| Hotz & Miller (1993) | CCP ä¼°è®¡ | â­â­â­â­ |
| DubÃ© et al. (2012) | MPEC æ–¹æ³• | â­â­â­ |

---

## ç»ˆç« ï¼šä¸€é¡µçº¸é€ŸæŸ¥è¡¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ç»“æ„ä¼°è®¡é€ŸæŸ¥è¡¨                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â–¸ æ ¸å¿ƒå…¬å¼                                                  â”‚
â”‚    Logitä»½é¢:    s_j = exp(Î´_j) / (1 + Î£ exp(Î´_k))          â”‚
â”‚    BLPåæ¼”:      Î´^{t+1} = Î´^t + log(s_obs) - log(s_pred)   â”‚
â”‚    GMMå‡†åˆ™:      Q(Î¸) = g(Î¸)' W g(Î¸)                        â”‚
â”‚    æœ€ä¼˜W:        W = [E(g g')]^{-1}                         â”‚
â”‚                                                             â”‚
â”‚  â–¸ è¯†åˆ«ä¸‰é—®                                                  â”‚
â”‚    1. å·¥å…·å˜é‡ä¸å†…ç”Ÿå˜é‡ç›¸å…³å—ï¼Ÿ(ç›¸å…³æ€§)                       â”‚
â”‚    2. å·¥å…·å˜é‡ä¸è¯¯å·®é¡¹ç‹¬ç«‹å—ï¼Ÿ(å¤–ç”Ÿæ€§)                        â”‚
â”‚    3. è¿‡åº¦è¯†åˆ«çº¦æŸå¯æ£€éªŒå—ï¼Ÿ(è¿‡åº¦è¯†åˆ«)                        â”‚
â”‚                                                             â”‚
â”‚  â–¸ Debugä¼˜å…ˆçº§                                               â”‚
â”‚    1. å†…å±‚å¾ªç¯æ”¶æ•›æ€§ (tol < 1e-12)                           â”‚
â”‚    2. ç›®æ ‡å‡½æ•°æ•°é‡çº§ (Q < 0.1 ä¸ºä½³)                          â”‚
â”‚    3. é¢„æµ‹ä»½é¢ç›¸å…³æ€§ (corr > 0.95)                           â”‚
â”‚    4. å¼¹æ€§ç¬¦å· (è‡ªä»·æ ¼ < 0)                                  â”‚
â”‚    5. å¤šèµ·å§‹ç‚¹ä¸€è‡´æ€§                                         â”‚
â”‚                                                             â”‚
â”‚  â–¸ ä»£ç æ£€æŸ¥æ¸…å•                                              â”‚
â”‚    â–¡ éšæœºç§å­å›ºå®š                                           â”‚
â”‚    â–¡ æ•°å€¼ç¨³å®š (log-sum-exp trick)                           â”‚
â”‚    â–¡ è¾¹ç•Œçº¦æŸåˆç†                                           â”‚
â”‚    â–¡ æ¢¯åº¦æ£€éªŒé€šè¿‡                                           â”‚
â”‚    â–¡ ç»“æœå¯å¤ç°                                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**ç¥ä½ åœ¨ç»“æ„ä¼°è®¡çš„é“è·¯ä¸Šä¸€è·¯é¡ºé£ï¼** ğŸ“

å¦‚æœæœ‰å…·ä½“é—®é¢˜ï¼ˆå¦‚æŸä¸ªå…¬å¼æ¨å¯¼ã€ä»£ç  bugã€æˆ–è€…ç‰¹å®šæ¨¡å‹çš„å®ç°ï¼‰ï¼Œæ¬¢è¿ç»§ç»­è®¨è®ºã€‚

---

## è¢«å¼•ç”¨æƒ…å†µ

- [åŠ¨æ€ä¼˜åŒ–ç»“æ„ä¼°è®¡å¤§å…¨](/posts/Dynamic/)
